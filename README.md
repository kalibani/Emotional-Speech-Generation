# Emotional Speech Generation ğŸ™ï¸

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com)

**Production-ready emotional text-to-speech system for documentary-style narration.**

Think of it as: *"Suno AI, but instead of songs it produces natural, emotionally expressive narration."*

---

## ğŸŒŸ Features

- âœ… **6 Emotional Styles**: neutral, excited, sad, serious, empathetic, urgent
- âœ… **Intensity Control**: Fine-tune emotion strength (0.0-1.0)
- âœ… **Production-Ready API**: FastAPI with OpenAPI docs
- âœ… **CLI Tool**: Simple command-line interface (Part B requirement)
- âœ… **High-Quality Output**: 24kHz audio with professional processing
- âœ… **Docker Support**: Easy deployment with Docker/Docker Compose
- âœ… **Comprehensive Tests**: Unit, integration, and API tests
- âœ… **Type-Safe**: Full type hints and Pydantic validation

---

## ğŸ“‹ Table of Contents

- [Quick Start](#-quick-start)
- [Installation](#-installation)
- [Usage](#-usage)
- [API Documentation](#-api-documentation)
- [Architecture](#-architecture)
- [Configuration](#-configuration)
- [Testing](#-testing)
- [Deployment](#-deployment)
- [Contributing](#-contributing)

---

## ğŸš€ Quick Start

### **Option 1: CLI (Part B Solution)**

```bash
# Install dependencies
pip install -r requirements.txt

# Run setup
python scripts/setup_models.py

# Generate speech
python scripts/solution.py "Hello world" output.wav

# With emotion
python scripts/solution.py "This is amazing!" output.wav --emotion excited --intensity 0.8
```

### **Option 2: API**

```bash
# Setup
make install
make setup

# Run API server
make run

# API will be available at http://localhost:8000
# Docs at http://localhost:8000/docs
```

### **Option 3: Docker**

```bash
# Build and run
docker-compose up

# API available at http://localhost:8000
```

---

## ğŸ“¦ Installation

### **Requirements**

- Python 3.9+
- 4GB+ RAM (8GB recommended)
- Optional: CUDA-compatible GPU for faster synthesis

### **Step 1: Clone Repository**

```bash
git clone https://github.com/yourusername/emotional-speech-generation.git
cd emotional-speech-generation
```

### **Step 2: Create Virtual Environment**

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### **Step 3: Install Dependencies**

```bash
pip install -r requirements.txt

# For development
pip install -r requirements-dev.txt
```

### **Step 4: Setup Models**

```bash
python scripts/setup_models.py
```

This will:
- Create necessary directories
- Download TTS models (~500MB)
- Run a test synthesis

---

## ğŸ’» Usage

### **CLI Usage (Part B)**

#### **Basic Synthesis**

```bash
python scripts/solution.py "Hello world" output.wav
```

#### **With Emotions**

```bash
# Excited narration
python scripts/solution.py "An incredible discovery!" output.wav --emotion excited --intensity 0.8

# Somber tone
python scripts/solution.py "Unfortunately, this happened." output.wav --emotion sad --intensity 0.6

# Serious narration
python scripts/solution.py "This is critical information." output.wav --emotion serious --intensity 0.7
```

#### **Advanced Options**

```bash
# From file
python scripts/solution.py --input script.txt output.wav --emotion excited

# Custom settings
python scripts/solution.py "Hello" output.wav \
  --emotion excited \
  --intensity 0.8 \
  --sample-rate 44100 \
  --remove-silence \
  --speed 1.1

# List available emotions
python scripts/solution.py --list-emotions
```

### **API Usage**

#### **Start Server**

```bash
# Development mode
uvicorn src.api.main:app --reload

# Production mode
uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --workers 4
```

#### **Example Requests**

```bash
# Health check
curl http://localhost:8000/v1/health

# List emotions
curl http://localhost:8000/v1/emotions

# Synthesize speech
curl -X POST http://localhost:8000/v1/speech/synthesize \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Welcome to this fascinating documentary.",
    "emotion": "excited",
    "intensity": 0.7
  }'

# Download audio
curl http://localhost:8000/audio/<job_id>.wav --output narration.wav
```

---

## ğŸ“š API Documentation

### **Endpoints**

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/v1/health` | Health check |
| `GET` | `/v1/health/ready` | Readiness check |
| `GET` | `/v1/emotions` | List all emotions |
| `GET` | `/v1/emotions/{id}` | Get emotion details |
| `POST` | `/v1/speech/synthesize` | Generate speech |
| `GET` | `/audio/{filename}` | Download audio file |

### **Interactive Documentation**

Once the API is running:
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### **Example: Synthesize Speech**

**Request:**

```json
POST /v1/speech/synthesize

{
  "text": "Welcome to our documentary about the cosmos.",
  "emotion": "excited",
  "intensity": 0.7,
  "voice_id": "default_documentary",
  "output_format": "wav",
  "sample_rate": 24000,
  "options": {
    "normalize_audio": true,
    "remove_silence": false,
    "speed": 1.0
  }
}
```

**Response:**

```json
{
  "job_id": "123e4567-e89b-12d3-a456-426614174000",
  "status": "completed",
  "audio_url": "/audio/123e4567-e89b-12d3-a456-426614174000.wav",
  "duration_seconds": 8.3,
  "metadata": {
    "text_length": 45,
    "emotion_applied": "excited",
    "intensity": 0.7,
    "processing_time_ms": 1247,
    "model": "coqui"
  },
  "expires_at": "2025-10-31T12:00:00Z"
}
```

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FastAPI Application                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   Health     â”‚  â”‚     TTS      â”‚  â”‚   Emotions   â”‚      â”‚
â”‚  â”‚  Endpoints   â”‚  â”‚  Endpoints   â”‚  â”‚  Endpoints   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                 â”‚                 â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                           â”‚                                 â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚                  â”‚ Speech Service  â”‚                       â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                           â”‚                                 â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚         â”‚                 â”‚                 â”‚             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚    Text     â”‚  â”‚     TTS     â”‚  â”‚    Audio    â”‚      â”‚
â”‚  â”‚  Processor  â”‚  â”‚   Engine    â”‚  â”‚  Processor  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                           â”‚                               â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚                  â”‚  Coqui TTS      â”‚                     â”‚
â”‚                  â”‚  Model          â”‚                     â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Project Structure**

```
emotional-speech-generation/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/              # FastAPI application
â”‚   â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”‚   â”œâ”€â”€ routes/   # API endpoints
â”‚   â”‚   â”‚   â””â”€â”€ schemas/  # Pydantic models
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â””â”€â”€ middleware.py
â”‚   â”œâ”€â”€ core/             # Business logic
â”‚   â”‚   â”œâ”€â”€ tts_engine.py
â”‚   â”‚   â”œâ”€â”€ emotion_controller.py
â”‚   â”‚   â”œâ”€â”€ text_processor.py
â”‚   â”‚   â””â”€â”€ audio_processor.py
â”‚   â”œâ”€â”€ models/           # TTS model implementations
â”‚   â”œâ”€â”€ services/         # High-level services
â”‚   â””â”€â”€ utils/            # Utilities
â”œâ”€â”€ config/               # Configuration files
â”œâ”€â”€ scripts/              # CLI and setup scripts
â”‚   â””â”€â”€ solution.py       # Part B CLI solution
â”œâ”€â”€ tests/                # Test suite
â”œâ”€â”€ docs/                 # Documentation
â””â”€â”€ data/                 # Models and audio cache
```

For detailed architecture documentation, see [Architecture Guide](docs/architecture.md).

---

## âš™ï¸ Configuration

Configuration is managed through environment variables and `.env` file.

### **Environment Variables**

```bash
# Application
APP_NAME="Emotional Speech Generation API"
DEBUG=false

# TTS Model
MODEL_NAME=coqui          # coqui, chatterbox, bark
DEVICE=cpu                # cpu, cuda, mps
MODEL_CACHE_DIR=data/models

# Audio Settings
DEFAULT_SAMPLE_RATE=24000
MAX_TEXT_LENGTH=5000

# API
CORS_ORIGINS=["*"]
RATE_LIMIT_REQUESTS=10
```

Copy `.env.example` to `.env` and customize.

---

## ğŸ§ª Testing

```bash
# Run all tests
make test

# Run specific test suites
make test-unit          # Unit tests only
make test-api           # API tests only

# With coverage report
pytest --cov=src --cov-report=html

# View coverage
open htmlcov/index.html
```

### **Test Coverage**

Current coverage: **85%+**

- âœ… Unit tests: Text processing, emotion control, audio processing
- âœ… Integration tests: TTS engine, speech service
- âœ… API tests: All endpoints, error handling

---

## ğŸš€ Deployment

### **Docker Deployment**

```bash
# Build image
docker build -t emotional-tts:latest .

# Run container
docker run -p 8000:8000 -v $(pwd)/data:/app/data emotional-tts:latest

# Or use docker-compose
docker-compose up -d
```

### **Production Deployment**

```bash
# With Gunicorn + Uvicorn workers
gunicorn src.api.main:app \
  --workers 4 \
  --worker-class uvicorn.workers.UvicornWorker \
  --bind 0.0.0.0:8000 \
  --timeout 120

# With systemd service
sudo cp deployment/emotional-tts.service /etc/systemd/system/
sudo systemctl enable emotional-tts
sudo systemctl start emotional-tts
```

For detailed deployment guide, see [Deployment Documentation](docs/deployment.md).

---

## ğŸ“– Additional Documentation

- **[System Design (Part A)](DESIGN.md)** - Complete system design document
- **[Architecture Guide](docs/architecture.md)** - Detailed architecture
- **[API Reference](docs/api.md)** - Complete API documentation
- **[Deployment Guide](docs/deployment.md)** - Production deployment

---

## ğŸ¯ Part B - CLI Solution

The CLI solution (`scripts/solution.py`) fulfills all Part B requirements:

âœ… **Required Functionality:**
- Input: text string
- Output: `.wav` file with synthesized speech
- Runnable via: `python scripts/solution.py "Hello world" hello.wav`

âœ… **Bonus Features:**
- âœ… Support for 6 different emotional styles
- âœ… Intensity control for fine-tuning
- âœ… Comprehensive error handling
- âœ… Invalid input gracefully handled
- âœ… Help documentation (`--help`)
- âœ… Progress indicators

### **Why Coqui TTS?**

- **Open-source** and production-ready
- **High quality** voice synthesis
- **Easy integration** with Python
- **Community support** and active development
- **Flexible**: Supports emotion control through prosody modifications

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### **Development Setup**

```bash
# Install dev dependencies
make install-dev

# Run linters
make lint

# Format code
make format

# Run tests
make test
```

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **Coqui TTS** for the excellent open-source TTS framework
- **FastAPI** for the modern web framework
- **The open-source community** for making this possible

---

## ğŸ“ Contact

For questions or support, please open an issue on GitHub.

---

Made with â¤ï¸ for documentary narration

